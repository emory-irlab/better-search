{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSbVyrpeiI2j",
        "outputId": "c8ed6755-3423-4dc3-fe3c-df11e1945d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: gradio==3.21.0 in /usr/local/lib/python3.10/dist-packages (3.21.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (23.2.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (3.8.6)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.104.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.25.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.19.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (1.23.5)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (3.9.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (9.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (1.10.13)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (4.8.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (0.24.0.post1)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.21.0) (12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.21.0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.21.0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.21.0) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio==3.21.0) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio==3.21.0) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio==3.21.0) (23.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.21.0) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.21.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.21.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.21.0) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.21.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.21.0) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.21.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.21.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.21.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.21.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.21.0) (1.3.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.21.0) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.21.0) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.21.0) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.21.0) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.21.0) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.21.0) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.21.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.21.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.21.0) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.21.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.21.0) (3.1.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.21.0) (2.0.7)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->gradio==3.21.0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->gradio==3.21.0) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.21.0) (1.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.21.0) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.21.0) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.21.0) (0.12.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.21.0) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio==3.21.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.5.3)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from python-terrier) (4.66.1)\n",
            "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
            "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.2)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
            "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from python-terrier) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
            "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
            "Requirement already satisfied: ir-datasets>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
            "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
            "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
            "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (4.11.2)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (2.3.2)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (4.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (6.0.1)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (4.3.2)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.3)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.6)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (3.2.3)\n",
            "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.12)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.2)\n",
            "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (2023.7.22)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.14.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
            "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->python-terrier) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->python-terrier) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->python-terrier) (3.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (23.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier) (1.0.0)\n",
            "Requirement already satisfied: coolname in /usr/local/lib/python3.10/dist-packages (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install gradio=='3.21.0'\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!pip install python-terrier\n",
        "!pip install coolname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-lt0LR1Hlnw_"
      },
      "outputs": [],
      "source": [
        "import pyterrier as pt\n",
        "from pyterrier.measures import * # don't uncomment this #ok\n",
        "import os\n",
        "import tqdm\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import coolname\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_seeds():\n",
        "    print(f'torch seed = {torch.seed()}')\n",
        "    print(f'numpy seed = {np.random.seed()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHs0WgUbM-c0",
        "outputId": "84635daf-b906-4fc9-82cb-8aad7d28c10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not pt.started():\n",
        "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2OiVRcIYNdSs"
      },
      "outputs": [],
      "source": [
        "def cleanup(s1):\n",
        "    return \"\".join([x if x.isalnum() else \" \" for x in s1.strip()])\n",
        "\n",
        "def get_index(EVALUATION_NAME, index_name, field=None, colbert=False,):\n",
        "    if index_name == \"vaswani\":\n",
        "        print(f\"Loading Index {index_name}...\")\n",
        "        index_path = f'./indices/{index_name}'\n",
        "        pt_index_path = index_path + '/data.properties'\n",
        "        if not os.path.exists(pt_index_path):\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            indexer = pt.IterDictIndexer(index_path)\n",
        "            index_ref = indexer.index(dataset.get_corpus_iter(), fields=['text'])\n",
        "        else:\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            print('Using prebuilt index.')\n",
        "            index_ref = pt.IndexRef.of(pt_index_path)\n",
        "        index = pt.IndexFactory.of(index_ref)\n",
        "        print('Completed indexing')\n",
        "        if colbert:\n",
        "            return index, dataset, dataset.get_topics(), dataset.get_corpus_iter\n",
        "        queries = dataset.get_topics()\n",
        "        queries['query'] = queries['query'].apply(cleanup)\n",
        "        return index, dataset, queries\n",
        "    if index_name == \"beir_dbpedia-entity\":\n",
        "        print(f\"Loading Index {index_name}...\")\n",
        "        index_path = f'./indices/{index_name}'\n",
        "        pt_index_path = index_path + '/data.properties'\n",
        "        if not os.path.exists(pt_index_path):\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            indexer = pt.IterDictIndexer(index_path, meta={\"docno\": 200})\n",
        "            index_ref = indexer.index(dataset.get_corpus_iter(), fields=['text', 'title', 'url'])\n",
        "        else:\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            print('Using prebuilt index.')\n",
        "            index_ref = pt.IndexRef.of(pt_index_path)\n",
        "        index = pt.IndexFactory.of(index_ref)\n",
        "        queries = dataset.get_topics()\n",
        "        queries['query'] = queries['query'].apply(cleanup)\n",
        "        print('Completed indexing')\n",
        "        if colbert:\n",
        "            def corpus_iterator():\n",
        "                for y in dataset.get_corpus_iter():\n",
        "                    y['text'] = y['title'] + \" \" + y['text']\n",
        "                    if y['text'].strip():\n",
        "                        yield y\n",
        "            return index, dataset, dataset.get_topics(), corpus_iterator\n",
        "        return index, dataset, queries\n",
        "    if index_name == \"beir_webis-touche2020_v2\":\n",
        "        print(f\"Loading Index {index_name}...\")\n",
        "        index_path = f'./indices/{index_name}'\n",
        "        pt_index_path = index_path + '/data.properties'\n",
        "        if not os.path.exists(pt_index_path):\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            indexer = pt.IterDictIndexer(index_path, meta={\"docno\": 39})\n",
        "            index_ref = indexer.index(dataset.get_corpus_iter(), fields=['text', 'title', 'stance', 'url'])\n",
        "        else:\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            print('Using prebuilt index.')\n",
        "            index_ref = pt.IndexRef.of(pt_index_path)\n",
        "        index = pt.IndexFactory.of(index_ref)\n",
        "        print('Completed indexing')\n",
        "        queries = dataset.get_topics()\n",
        "        queries['query'] = queries['description'].str.cat(queries['text'], sep=' ')\n",
        "        queries['query'] = queries['query'].apply(cleanup)\n",
        "        if colbert:\n",
        "            def corpus_iterator():\n",
        "                for y in dataset.get_corpus_iter():\n",
        "                    y['text'] = y['title'] + \" \" + y['text']\n",
        "                    if y['text'].strip():\n",
        "                        yield y\n",
        "            return index, dataset, queries, corpus_iterator\n",
        "        return index, dataset, queries\n",
        "    elif index_name == \"msmarco_passage\":\n",
        "        print(f\"Loading Index {index_name}...\")\n",
        "        index_path = f'./indices/{index_name}'\n",
        "        pt_index_path = index_path + '/data.properties'\n",
        "        if not os.path.exists(pt_index_path):\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            indexer = pt.IterDictIndexer(index_path)\n",
        "            index_ref = indexer.index(dataset.get_corpus_iter(), fields=['text'])\n",
        "        else:\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            print('Using prebuilt index.')\n",
        "            index_ref = pt.IndexRef.of(pt_index_path)\n",
        "        index = pt.IndexFactory.of(index_ref)\n",
        "        print('Completed indexing')\n",
        "        if colbert:\n",
        "            return index, dataset, dataset.get_topics(), dataset.get_corpus_iter\n",
        "        queries = dataset.get_topics()\n",
        "        queries['query'] = queries['query'].apply(cleanup)\n",
        "        return index, dataset, queries\n",
        "    elif index_name == \"msmarco_document\":\n",
        "        print(f\"Loading Index {index_name}...\")\n",
        "        index_path = f'./indices/{index_name}'\n",
        "        pt_index_path = index_path + '/data.properties'\n",
        "        if not os.path.exists(pt_index_path):\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            indexer = pt.IterDictIndexer(index_path)\n",
        "            index_ref = indexer.index(dataset.get_corpus_iter(), fields=['url', 'title', 'body'])\n",
        "        else:\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            print('Using prebuilt index.')\n",
        "            index_ref = pt.IndexRef.of(index_path)\n",
        "        index = pt.IndexFactory.of(index_ref)\n",
        "        print('Completed indexing')\n",
        "        queries = dataset.get_topics()\n",
        "        queries['query'] = queries['query'].apply(cleanup)\n",
        "        return index, dataset, queries\n",
        "    elif index_name == \"trec-covid\":\n",
        "        print(f\"Loading Index {index_name}...\")\n",
        "        EVALUATION_NAME = \"irds:cord19/trec-covid\"\n",
        "        index_name = \"cord19/trec-covid\"\n",
        "        index_path = f'./indices/{index_name}'\n",
        "        pt_index_path = index_path + '/data.properties'\n",
        "        if not os.path.exists(pt_index_path):\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            indexer = pt.IterDictIndexer(index_path)\n",
        "            index_ref = indexer.index(dataset.get_corpus_iter(), fields=['title', 'doi', 'date', 'abstract'])\n",
        "        else:\n",
        "            dataset = pt.get_dataset(EVALUATION_NAME)\n",
        "            print('Using prebuilt index.')\n",
        "            index_ref = pt.IndexRef.of(index_path)\n",
        "        index = pt.IndexFactory.of(index_ref)\n",
        "        print('Completed indexing')\n",
        "        queries = dataset.get_topics()\n",
        "        queries['query'] = queries['title'].str.cat(queries['description'], sep=' ')\n",
        "        queries['query'] = queries['query'].apply(lambda text: text.replace(\"?\", \"\"))\n",
        "        queries['query'] = queries['query'].apply(cleanup)\n",
        "        if colbert:\n",
        "            def corpus_iterator():\n",
        "                for y in dataset.get_corpus_iter():\n",
        "                    y['text'] = y['title'] + \" \" + y['abstract']\n",
        "                    if y['text'].strip():\n",
        "                        yield y\n",
        "            return index, dataset, queries, corpus_iterator\n",
        "        return index, dataset, queries\n",
        "    else:\n",
        "        print(f\"KD:No index selected of name {index_name}.\")\n",
        "        return None\n",
        "\n",
        "def get_bm25_pipe(index_name, index):\n",
        "    if index_name in [\"trec-covid\", \"msmarco_passage\", \"msmarco_document\"]:\n",
        "        bm25 = pt.BatchRetrieve.from_dataset(index_name, 'terrier_stemmed', wmodel='BM25')\n",
        "        #bm25_10000 = pt.BatchRetrieve.from_dataset(index_name, 'terrier_stemmed', wmodel='BM25', num_results=10000)\n",
        "    else:\n",
        "        bm25 = pt.BatchRetrieve(index, wmodel='BM25')\n",
        "        #bm25_10000 = pt.BatchRetrieve.from_dataset(index_name, 'terrier_stemmed', wmodel='BM25', num_results=10000)\n",
        "    return bm25\n",
        "\n",
        "triplets = [\n",
        "['irds:msmarco-passage/trec-dl-2019/judged',  'msmarco_passage', 'text', 'text'],\n",
        "[\"irds:beir/webis-touche2020/v2\", \"beir_webis-touche2020_v2\", \"text\", \"text\"],\n",
        "[\"irds:beir/dbpedia-entity/test\", \"beir_dbpedia-entity\", 'text', 'text'],\n",
        "[\"irds:vaswani\", \"vaswani\", 'text', 'text']]\n",
        "\n",
        "bm25= None; tfidf= None; docno2doctext = None\n",
        "def on_dataset_change(dataset_name):\n",
        "  triplet = [t for t in triplets if t[1]==dataset_name][0]\n",
        "  EVALUATION_NAME = triplet[0]; index_name = triplet[1]; field = triplet[2]; doc_field = triplet[3]\n",
        "  index, dataset, queries, corpus_iterator = get_index(EVALUATION_NAME, index_name, field, colbert=True)\n",
        "  docno2doctext = {doc['docno']: doc[field] for doc in corpus_iterator()}\n",
        "  bm25 = pt.BatchRetrieve(index, wmodel='BM25')\n",
        "  tfidf = pt.BatchRetrieve(index, wmodel='TF_IDF')\n",
        "  return bm25, tfidf, docno2doctext\n",
        "\n",
        "def user_selects_different_index(index_id): # for dropdpwn\n",
        "  triplet = triplets[index_id]\n",
        "  return on_dataset_change(triplet[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fw5csaI_6JxR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "SAVED_DATA_DIRECTORY = \"saved_data\"\n",
        "if not os.path.exists(SAVED_DATA_DIRECTORY):\n",
        "  os.mkdir(SAVED_DATA_DIRECTORY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "67cb9f6a66e7475eb029e0e21c778fb6",
            "394ed19f676e436f9d323d50b17e2e27",
            "1d4cbc87ab024c6e8908121b997fc2c4",
            "072b7bdae8e84efd9e55a805181a7048",
            "72c9f93238e9447c8ef5b1f661bc12fc",
            "7e3bc5373fd946e89845351b81e64bfa",
            "d8783bced142490bb60e31725c02e0c6",
            "8660ac017a6a4c14806e200aca11dd0a",
            "f4d06a5141434d80a7bccf4a4aaea2f0",
            "e48b5e3564cd4424a85bd7f9dd357a3b",
            "da2671c6f4664171923ea4b598b54357"
          ]
        },
        "id": "-MNJPCG3QBe5",
        "outputId": "e1aec317-c1a1-4b77-b623-ebcb63a787b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Index msmarco_passage...\n",
            "Using prebuilt index.\n",
            "Completed indexing\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "msmarco-passage/trec-dl-2019/judged documents:   0%|          | 0/8841823 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67cb9f6a66e7475eb029e0e21c778fb6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset_names = [triplet[1] for triplet in triplets]\n",
        "# default is id=3\n",
        "bm25, tfidf, docno2doctext  = user_selects_different_index(3) # vaswani is the fastest to load. For testing use index_id = 3 (vaswani)\n",
        "retrieval_algos_dict = {'BM25': bm25, 'TF_IDF': tfidf}\n",
        "retrieval_algos_names = ['BM25','TF_IDF']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MssZ0jfSUTIo"
      },
      "outputs": [],
      "source": [
        "def get_doc_text(docno):\n",
        "  if docno not in docno2doctext.keys():\n",
        "    return f\"Document Text not found for Document ID = {docno}\"\n",
        "  return docno2doctext[docno]\n",
        "\n",
        "def retrieve_for_ui(query_text, pipeline):\n",
        "  searchresults1 = (pipeline%10).search(cleanup(query_text))\n",
        "  searchresults1['eng-text'] = searchresults1['docno'].apply(get_doc_text)\n",
        "  searchresults1['target-text'] = searchresults1['eng-text']\n",
        "  res = [row.to_dict() for index, row in searchresults1.iterrows()]\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4io17vV3bVx",
        "outputId": "d8886179-8ce9-444b-9263-21dca4c53a25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'qid': '1',\n",
              "  'docid': 130866,\n",
              "  'docno': '130866',\n",
              "  'rank': 0,\n",
              "  'score': 32.4647296129182,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': 'Where is Afghanistan? GDP of Afghanistan; What is the Capital of Afghanistan? List of Airports in Afghanistan',\n",
              "  'target-text': 'Where is Afghanistan? GDP of Afghanistan; What is the Capital of Afghanistan? List of Airports in Afghanistan'},\n",
              " {'qid': '1',\n",
              "  'docid': 2552501,\n",
              "  'docno': '2552501',\n",
              "  'rank': 1,\n",
              "  'score': 28.037573297954083,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': \"For the capital city of Afghanistan, see Kabul. Kābul (Pashto: کابل Kābəl\\u200e, Persian: کابل Kābol\\u200e\\u200e), situated in the east of the country, is one of the thirty-four provinces of Afghanistan. The capital of the province is Kabul city, which is also Afghanistan's capital. The population of the Kabul Province is nearly 4 million people as of 2012, of which almost 80 percent live in the urban areas. The current governor of the province is Hamid Akram.\",\n",
              "  'target-text': \"For the capital city of Afghanistan, see Kabul. Kābul (Pashto: کابل Kābəl\\u200e, Persian: کابل Kābol\\u200e\\u200e), situated in the east of the country, is one of the thirty-four provinces of Afghanistan. The capital of the province is Kabul city, which is also Afghanistan's capital. The population of the Kabul Province is nearly 4 million people as of 2012, of which almost 80 percent live in the urban areas. The current governor of the province is Hamid Akram.\"},\n",
              " {'qid': '1',\n",
              "  'docid': 1484396,\n",
              "  'docno': '1484396',\n",
              "  'rank': 2,\n",
              "  'score': 27.878888994521848,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': 'This article is about a province of Afghanistan. For its capital city, see Herat. Herat (Pashto/Dari: هرات) is one of the thirty-four provinces of Afghanistan, located in the western part of the country. Together with Badghis, Farah, and Ghor provinces, it makes up the south-western region of Afghanistan. Its primary city and administrative capital is Herat City.',\n",
              "  'target-text': 'This article is about a province of Afghanistan. For its capital city, see Herat. Herat (Pashto/Dari: هرات) is one of the thirty-four provinces of Afghanistan, located in the western part of the country. Together with Badghis, Farah, and Ghor provinces, it makes up the south-western region of Afghanistan. Its primary city and administrative capital is Herat City.'},\n",
              " {'qid': '1',\n",
              "  'docid': 4662621,\n",
              "  'docno': '4662621',\n",
              "  'rank': 3,\n",
              "  'score': 27.35803800573543,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': \"Kabul Province. Kābul (Pashto: کابل Kābəl, Persian: کابل Kābol), situated in the east of the country, is one of the thirty-four provinces of Afghanistan. The capital of the province is Kabul City, which is also Afghanistan's capital. There are about 3.5 million people living in the province as of 2009.\",\n",
              "  'target-text': \"Kabul Province. Kābul (Pashto: کابل Kābəl, Persian: کابل Kābol), situated in the east of the country, is one of the thirty-four provinces of Afghanistan. The capital of the province is Kabul City, which is also Afghanistan's capital. There are about 3.5 million people living in the province as of 2009.\"},\n",
              " {'qid': '1',\n",
              "  'docid': 6124118,\n",
              "  'docno': '6124118',\n",
              "  'rank': 4,\n",
              "  'score': 27.295219659920722,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': 'Its located in East Afghanistan its also the capital city of Afghanistan and the best place in the world and its where I have come from.   http://en.wikipedia.org/wiki/Kabu … l.',\n",
              "  'target-text': 'Its located in East Afghanistan its also the capital city of Afghanistan and the best place in the world and its where I have come from.   http://en.wikipedia.org/wiki/Kabu … l.'},\n",
              " {'qid': '1',\n",
              "  'docid': 4662620,\n",
              "  'docno': '4662620',\n",
              "  'rank': 5,\n",
              "  'score': 26.621684980141644,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': 'Kabul Province. Kabul situated in the east of the country, is one of the thirty-four provinces of Afghanistan. The capital of the province is Kabul city, which is also Afghanistan’s capital. The population of the Kabul Province is nearly 4 million people as of 2012, of which almost 80 percent live in the urban areas.',\n",
              "  'target-text': 'Kabul Province. Kabul situated in the east of the country, is one of the thirty-four provinces of Afghanistan. The capital of the province is Kabul city, which is also Afghanistan’s capital. The population of the Kabul Province is nearly 4 million people as of 2012, of which almost 80 percent live in the urban areas.'},\n",
              " {'qid': '1',\n",
              "  'docid': 5259701,\n",
              "  'docno': '5259701',\n",
              "  'rank': 6,\n",
              "  'score': 26.434889734295126,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': 'The Taliban (Pashto: طالبان \\u200e ṭālibān students), alternately spelled Taleban, is an Islamic fundamentalist political movement in Afghanistan. It spread throughout Afghanistan and formed a government, ruling as the Islamic Emirate of Afghanistan from September 1996 until December 2001, with Kandahar as the capital.',\n",
              "  'target-text': 'The Taliban (Pashto: طالبان \\u200e ṭālibān students), alternately spelled Taleban, is an Islamic fundamentalist political movement in Afghanistan. It spread throughout Afghanistan and formed a government, ruling as the Islamic Emirate of Afghanistan from September 1996 until December 2001, with Kandahar as the capital.'},\n",
              " {'qid': '1',\n",
              "  'docid': 3895438,\n",
              "  'docno': '3895438',\n",
              "  'rank': 7,\n",
              "  'score': 26.422533684027798,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': 'Afghanistan /aefˈɡaenɨstaen/ (æfˈɡænɨstæn / Pashto: dari, افغانستان), afġānistān officially The Islamic republic Of, afghanistan is a landlocked country located Within South asia And Central. asian October 1772, Durrani died of a natural cause and was buried at a site now adjacent to the Shrine of the Cloak in Kandahar. He was succeeded by his son, Timur Shah, who transferred the capital of Afghanistan from Kandahar to Kabul in 1776.',\n",
              "  'target-text': 'Afghanistan /aefˈɡaenɨstaen/ (æfˈɡænɨstæn / Pashto: dari, افغانستان), afġānistān officially The Islamic republic Of, afghanistan is a landlocked country located Within South asia And Central. asian October 1772, Durrani died of a natural cause and was buried at a site now adjacent to the Shrine of the Cloak in Kandahar. He was succeeded by his son, Timur Shah, who transferred the capital of Afghanistan from Kandahar to Kabul in 1776.'},\n",
              " {'qid': '1',\n",
              "  'docid': 932085,\n",
              "  'docno': '932085',\n",
              "  'rank': 8,\n",
              "  'score': 26.240468450505812,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': \"Most of the Hazaras are Shi'ite Muslims, and, the 1% of the population which are not Muslims, are either Hindus, Sikhs, or Jews. The History of U.S. Involvement in Afghanistan:  The United States involvement in the country of Afghanistan really first began in 1934. This was the year the United States officially recognized Afghanistan's independence and set up an embassy in Kabul, Afghanistan's capital..\",\n",
              "  'target-text': \"Most of the Hazaras are Shi'ite Muslims, and, the 1% of the population which are not Muslims, are either Hindus, Sikhs, or Jews. The History of U.S. Involvement in Afghanistan:  The United States involvement in the country of Afghanistan really first began in 1934. This was the year the United States officially recognized Afghanistan's independence and set up an embassy in Kabul, Afghanistan's capital..\"},\n",
              " {'qid': '1',\n",
              "  'docid': 3895439,\n",
              "  'docno': '3895439',\n",
              "  'rank': 9,\n",
              "  'score': 25.806992472677834,\n",
              "  'query': 'who is the capital of afghanistan',\n",
              "  'eng-text': 'The political history of the modern state of Afghanistan began with the Hotak and Durrani dynasties in the 18th century. In the late 19th century, Afghanistan became a buffer state in the  Great Game  between British India and the Russian Empire.n October 1772, Durrani died of a natural cause and was buried at a site now adjacent to the Shrine of the Cloak in Kandahar. He was succeeded by his son, Timur Shah, who transferred the capital of Afghanistan from Kandahar to Kabul in 1776.',\n",
              "  'target-text': 'The political history of the modern state of Afghanistan began with the Hotak and Durrani dynasties in the 18th century. In the late 19th century, Afghanistan became a buffer state in the  Great Game  between British India and the Russian Empire.n October 1772, Durrani died of a natural cause and was buried at a site now adjacent to the Shrine of the Cloak in Kandahar. He was succeeded by his son, Timur Shah, who transferred the capital of Afghanistan from Kandahar to Kabul in 1776.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "retrieve_for_ui('who is the capital of afghanistan', bm25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NMrUSYQ6WOMJ"
      },
      "outputs": [],
      "source": [
        "pdd1 = retrieve_for_ui('some search query', bm25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rTdejOKKjBLh"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "# for batch size = 1\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import (\n",
        "    BeamSearchScorer,\n",
        "    LogitsProcessorList,\n",
        "    StoppingCriteriaList,\n",
        "    MaxLengthCriteria,\n",
        "    ForcedBOSTokenLogitsProcessor,\n",
        "    HammingDiversityLogitsProcessor, MinLengthLogitsProcessor\n",
        ")\n",
        "\n",
        "def clean1( text):\n",
        "    text = text.replace('<pad>', '')\n",
        "    text = text.replace('</s>', '')\n",
        "    text = text.strip().capitalize()\n",
        "    if text.endswith('?'):\n",
        "        return text\n",
        "    else:\n",
        "        return text + \"?\"\n",
        "\n",
        "class DiverseGenerator(object):\n",
        "    def __init__(self, forced_bos=True, hamming=True, model_name='castorini/doc2query-t5-base-msmarco', start_words=['What','When','Which','Where','How']):\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.model.to(device)\n",
        "        self.start_words = start_words\n",
        "        self.forced_bos = forced_bos\n",
        "        self.hamming = hamming\n",
        "    def clean(self, text):\n",
        "        return clean1(text)\n",
        "    def diverse_generate(self, document, num_beams = 20, basic_beam_search=True, hamming=False):\n",
        "        document_tokenized = self.tokenizer(document, return_tensors='pt')\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f'dev:{device}')\n",
        "        encoder_input_ids = document_tokenized['input_ids'].to(device)\n",
        "        generations = []\n",
        "        if basic_beam_search:\n",
        "            generated_ids = self.model.generate(encoder_input_ids, max_length=128,\n",
        "                                           pad_token_id=self.tokenizer.eos_token_id, num_beams=num_beams, num_return_sequences=5, temperature = 1.3,top_p=0.92, repetition_penalty =2.1, do_sample=True).tolist()\n",
        "            preds = [self.clean(self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True, )) for g in generated_ids]\n",
        "            generations.extend(preds)\n",
        "        input_ids = torch.ones((num_beams, 1), device=self.model.device, dtype=torch.long)\n",
        "        input_ids = input_ids * self.model.config.decoder_start_token_id\n",
        "        if hamming:\n",
        "            generations.extend(self.hamming_diverse(encoder_input_ids, input_ids,))\n",
        "        return generations\n",
        "    def hamming_diverse(self, encoder_input_ids, input_ids, num_beams = 6, num_beam_groups=3):\n",
        "        model_kwargs = {\n",
        "            \"encoder_outputs\": self.model.get_encoder()(\n",
        "                encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
        "            )\n",
        "        }\n",
        "        # instantiate beam scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=1,\n",
        "            max_length=self.model.config.max_length,\n",
        "            num_beams=num_beams,\n",
        "            num_beam_hyps_to_keep=num_beams,\n",
        "            device=self.model.device,\n",
        "            num_beam_groups=num_beam_groups,\n",
        "        )\n",
        "        logits_processor = LogitsProcessorList(\n",
        "            [HammingDiversityLogitsProcessor(5.5, num_beams=num_beams, num_beam_groups=num_beam_groups),\n",
        "             MinLengthLogitsProcessor(8, eos_token_id=self.model.config.eos_token_id),\n",
        "             ]\n",
        "        )\n",
        "        outputs = self.model.group_beam_search(\n",
        "            input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs\n",
        "        )\n",
        "        generations = []\n",
        "        for gen in self.tokenizer.batch_decode(outputs, skip_special_tokens=True):\n",
        "            generations.append(self.clean(gen))\n",
        "        #print(len(generations))\n",
        "        return generations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qHeFrNI3iLTp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def highlighted_doc(d):\n",
        "    #print(f\"D={str(d)}\")\n",
        "    document = d['eng-text']\n",
        "    event_str = d['event_str']\n",
        "    event_word_to_event = {e[0]['string']: str(e[0]['label']) for e in event_str}\n",
        "    if len(event_word_to_event)==0:\n",
        "        return []\n",
        "    highlights = []\n",
        "    for word in document.split(\" \"):\n",
        "        if word in event_word_to_event.keys():\n",
        "            event_type = event_word_to_event[word]\n",
        "            highlights.extend([(word, event_type)])\n",
        "        else:\n",
        "            highlights.extend([(word, None)])\n",
        "    return highlights\n",
        "\n",
        "def lang_to_emoji(lang):\n",
        "    if lang == \"fas\":\n",
        "        return \"🇮🇷 \"\n",
        "    if lang == \"eng\":\n",
        "        return \"🇺🇸 \"\n",
        "    if lang == 'kor':\n",
        "        return \"🇰🇷 \"\n",
        "    if lang == 'rus':\n",
        "        return \"🇷🇺 \"\n",
        "    if lang == 'zho':\n",
        "        return \"🇨🇳 \"\n",
        "    if lang =='ara':\n",
        "        return '🇸🇦 '\n",
        "    return \"📜\"\n",
        "\n",
        "NDOCS = 10\n",
        "max_text_lim = 400\n",
        "## need to update this with retrival method choosing\n",
        "\n",
        "def document_retriever(session, query_text,logs_file_name, model_choice_retrival=retrieval_algos_names[0], max_text_lim=400):\n",
        "    if logs_file_name != \"manual_search\":\n",
        "      on_query_change(query_text,logs_file_name, session)\n",
        "    retrieval_algo = retrieval_algos_dict[model_choice_retrival]\n",
        "    docs_all_lang = retrieve_for_ui(query_text, retrieval_algo)\n",
        "    #print(f\"docs_all_lang={docs_all_lang}\")\n",
        "    #print(f\"len(docs_all_lang) = {len(docs_all_lang)}\")\n",
        "    #print(f\"event_ann = {event_ann}\")\n",
        "    docs_to_ui = []\n",
        "    docs_en = [d['eng-text']  for d in docs_all_lang]\n",
        "    #print(f\"docs_en[0] = \" + docs_en[0])\n",
        "    #print(f\"len(docs_en) = {len(docs_en)}\")\n",
        "    docs_fas = [d['target-text'] for d in docs_all_lang]\n",
        "    docs_score = [d['score'] for d in docs_all_lang]\n",
        "    #print(f\"docs_fas[0] = \" + docs_fas[0])\n",
        "    #docs_fas = [str(d['events']).strip()[:500].strip() + \"...\" for d in event_ann]\n",
        "    docs_events = [\"\" for d in docs_en] # highlighted_doc(d)\n",
        "    for i in range(len(docs_en)):\n",
        "        #print(f\"i={str(i)}\")\n",
        "        #docs_to_ui.append(\"Title: \" + docs_title_dummy[i]) #lang_to_emoji(docs_fas[i]['lang'])+\n",
        "        dtran = str(docs_fas[i]).strip()[:max_text_lim].strip() + \"...\"\n",
        "        docs_to_ui.append(dtran)\n",
        "        deng = \"🇺🇸  \"+str(docs_en[i]).strip()[:max_text_lim].strip() + \"...\"\n",
        "        docs_to_ui.append(deng)\n",
        "        docs_to_ui.append(docs_score[i])\n",
        "        docs_to_ui.append(docs_events[i])\n",
        "    for i in range(len(docs_en), NDOCS):\n",
        "        docs_to_ui.append(\"\")\n",
        "        docs_to_ui.append(\"\")\n",
        "        docs_to_ui.append(\"\")\n",
        "        docs_to_ui.append(\"\")\n",
        "    if docs_to_ui is not None:\n",
        "        if len(docs_to_ui) == NDOCS*4:\n",
        "            return docs_to_ui\n",
        "    return [\"\"]*NDOCS*4\n",
        "\n",
        "def add_to_new_query(current_text_box, to_add):\n",
        "    return current_text_box + \" \" + to_add\n",
        "\n",
        "eg1=\"Eleven people were killed in a train crash in northern Italy...\"\n",
        "eg2=\"Based on the U.S. Electronics Network (CNN) cited Peruvian...\"\n",
        "def sample_doc_examples(inshort):\n",
        "    if inshort == eg1:\n",
        "        return \"Eleven people were killed in a train crash in northern Italy when the train crashed in the north of Italy The number of victims of the train disaster in northern Italy has grown to 11 people reported by the head of the administration of the autonomous province of Bolzano Louis Durnwalder.\"\n",
        "    if inshort == eg2:\n",
        "        return \"Based on the U.S. Electronics Network (CNN) cited Peruvian officials as saying that it is currently known that 2 people have died and 65 have been injured and the death toll has risen to 1 person.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rBuIw1BLyFZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4a769e-a2d3-464a-8c84-ca041b8be104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_name0 = 'google/flan-t5-large'\n",
        "diverseGenerator = DiverseGenerator(model_name=model_name0)\n",
        "\n",
        "def update_model(model_name):\n",
        "  if model_name is not model_name0:\n",
        "    print(f\"Updating query generators to {model_name}\")\n",
        "    diverseGenerator = DiverseGenerator(model_name=model_name0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Pbh6WjugmIbj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# basic beam search\n",
        "#this is what we are using now to generate queries, kd pls change it if you want in gradio\n",
        "def query_generator1(prompt_instruction,prompt_context, prompt_input, model_name, session):\n",
        "    input = prompt_instruction + \"\\n\" + prompt_context+\"\\nDocument:\"+prompt_input+\"\\nQuery:\"\n",
        "    queries = diverseGenerator.diverse_generate(input)\n",
        "    queries = list(set(queries))\n",
        "    #random.shuffle(queries)\n",
        "    #queries.sort(key=len, reverse=True)\n",
        "    if len(queries) >= 5:\n",
        "        return session,*queries[0:5]\n",
        "    else:\n",
        "        ll = [\"\"] * 5\n",
        "        for (i,q) in enumerate(queries):\n",
        "            ll[i] = q\n",
        "        return session,*ll\n",
        "\n",
        "def query_generatortemp(prompt_instruction,prompt_context, prompt_input, model_name, session):\n",
        "    input = prompt_instruction + \"\\n\" + prompt_context+\"\\nDocument:\"+prompt_input+\"\\nQuery:\"\n",
        "    queries = diverseGenerator.diverse_generate(input)\n",
        "    queries = list(set(queries))\n",
        "    #random.shuffle(queries)\n",
        "    #queries.sort(key=len, reverse=True)\n",
        "    if len(queries) >= 5:\n",
        "        return session,*queries[0:5]\n",
        "    else:\n",
        "        ll = [\"\"] * 5\n",
        "        for (i,q) in enumerate(queries):\n",
        "            ll[i] = q\n",
        "        return session,*ll\n",
        "# hamming diversity beam search\n",
        "def query_generator2(input,model_name):\n",
        "    diverseGenerator = DiverseGenerator(model_name=model_name)\n",
        "    queries = diverseGenerator.diverse_generate(input, basic_beam_search=False, hamming=True)\n",
        "    queries = list(set(queries))\n",
        "    #random.shuffle(queries)\n",
        "    #queries.sort(key=len, reverse=True)\n",
        "    if len(queries) >= 5:\n",
        "        return queries[0:5]\n",
        "    else:\n",
        "        ll = [\"\"] * 5\n",
        "        for (i,q) in enumerate(queries):\n",
        "            ll[i] = q\n",
        "        return ll\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A9o7x4HBfcLx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "#saving docs\n",
        "def save_document_auto(session, model_name,file_name,query, *document_texts):\n",
        "    relevance_annotations = []\n",
        "    for i in range(0, len(document_texts), 4):\n",
        "        text1 = document_texts[i]\n",
        "        if(text1):\n",
        "            text2 = document_texts[i + 1]\n",
        "            score = document_texts[i + 2]\n",
        "            rating = document_texts[i + 3]\n",
        "            if not (rating):\n",
        "                rating = 0\n",
        "            id = i // 4 + 1\n",
        "            relevance_annotations.append({\"id\": id, \"doc_test\": text1, \"translation\": text2, \"score\":score,\"rating\": rating})\n",
        "    final_data = {\n",
        "        \"session\": session,\n",
        "        \"query\": query,\n",
        "        \"model_choice\":model_name,\n",
        "        \"relevance_annotations\": relevance_annotations,\n",
        "    }\n",
        "    folder_path = 'saved_data'\n",
        "    file_path = f'{folder_path}/{file_name}.json'\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        with open(file_path, \"r\") as jsonfile:\n",
        "            data = json.load(jsonfile)\n",
        "            if isinstance(data, list):\n",
        "                data.append(final_data)\n",
        "            else:\n",
        "                data = [data, final_data]\n",
        "        with open(file_path, \"w\") as jsonfile:\n",
        "            json.dump(data, jsonfile, indent=4)\n",
        "    else:\n",
        "        with open(file_path, \"w\") as jsonfile:\n",
        "            json.dump([final_data], jsonfile, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P7tnyXs8na-H"
      },
      "outputs": [],
      "source": [
        "instructions = [\"Generate a query which is relevant to a given document\",\n",
        "                \"Generate a query which is relevant to a given document and is different from previously generated query\",\n",
        "                \"Generate a document relevant query with different words from the document\"]\n",
        "instruction = instructions[0]\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "def get_flant5xl():\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
        "    return tokenizer, model\n",
        "\n",
        "def dqpairs(documents, queries):\n",
        "  dqs = \"\\n\".join([f\"Document:{d}\\nQuery:{q}\" for (d,q) in zip(documents, queries)])\n",
        "  #dqs = f\"{instruction}:\\n\" + dqs\n",
        "  return dqs\n",
        "\n",
        "def dqprompt(documents, queries):\n",
        "  all_except_last = documents[:-1]\n",
        "  dqs = \"\\n\".join([f\"Document:{d}\\nQuery:{q}\" for (d,q) in zip(all_except_last, queries)])\n",
        "  #dqs = f\"{instruction}:\\n\" + dqs\n",
        "  dqs = dqs + f\"\\nDocument:{documents[-1]}\"\n",
        "  return dqs\n",
        "\n",
        "def get_dqlists_from_prompt(prompt_text):\n",
        "  dqs = prompt_text.split('\\n')\n",
        "  instruction = dqs[0]\n",
        "  ds = []\n",
        "  qs = []\n",
        "  for i in range(1, len(dqs) - 2, 2):\n",
        "    ds.append(dqs[i].split(\":\")[1])\n",
        "    qs.append(dqs[i+1].split(\":\")[1])\n",
        "  last_doc_not_needed = dqs[i].split(\":\")[1]\n",
        "  return ds, qs, instruction\n",
        "\n",
        "def update_prompt(model_input, document):\n",
        "    document = document.replace(\"🇺🇸\",\"\").strip()\n",
        "    ds, qs, instruction = get_dqlists_from_prompt(model_input)\n",
        "    # now keep last 2 doc-query pairs\n",
        "    if len(ds) > 2:\n",
        "        ds = ds[len(ds)-2:]\n",
        "        qs = qs[len(ds)-2:]\n",
        "    ds.append(document)\n",
        "    return instruction +\"\\n\"+ dqprompt(ds, qs) + \"\\nQuery:\"\n",
        "\n",
        "def send_feedback(input_query, document, model_choice_query, session, file_name=\"feedback_query_reformulations\"):\n",
        "  document = document.replace(\"🇺🇸\",\"\").strip()\n",
        "  rf_queries = query_generator1(f\"Based on the given context ```{document}```, generate keywords for the query : \",\"\", input_query,model_choice_query, session)\n",
        "  ref_query = input_query + \" \" + rf_queries[1]\n",
        "  on_query_change(ref_query, file_name, session)\n",
        "  return ref_query\n",
        "\n",
        "def append_keywords(session, query, reform_method, reform_instruction, model_choice_query, file_name=\"query_reformulations\"):\n",
        "  rf_queries = query_generator1(f\"Generate keywords for the query : \",\"\", query, model_choice_query, session)\n",
        "  ref_query = query + \" \" + rf_queries[1]\n",
        "  on_query_change(ref_query, file_name, session)\n",
        "  return ref_query\n",
        "\n",
        "ds_eg = [\"24 Aug 2016 At least 38 people died in a powerful earthquake that hit central Italy early on Wednesday 250 people are now known to have died in the earthquake that hit central Italy on Wednesday\",\n",
        "      \"As of Thursday morning, the deaths totaled 241, officials said. 6.2 Earthquake In Central Italy, At Least 37 Dead\"]\n",
        "qs_eg = [\"When did the earthquake happen?\", \"How many people died in the earthquake?\"]\n",
        "\n",
        "sample_docs = [\"China’s state-run Citic Group, the main developer of the project, said negotiations were ongoing and that the $1.3bn was to be spent on the “initial phase” of the port, adding the project was divided into four phases. It did not elaborate on plans for subsequent stages. Xi and Mitsotakis will visit the port of Piraeus, Greece’s largest and majority owned by Chinese port operator Cosco. It is the biggest Chinese investment in Greece and Cosco recently received approval for a new investment plan that includes building a new cargo terminal.\",\n",
        "               \"FireEye, one of the largest cyber security companies in the United States, said on Tuesday that it had been hacked, likely by a government, and that an arsenal of hacking tools used to test the defences of its clients had been stolen.  The hack of FireEye, a company with an array of contracts across the national security space both in the United States and its allies, is among the most significant breaches in recent memory.\",\n",
        "               \"The alleged state-backed hacking groups engaging in these attacks include a group from Russia code-named ‘Strontium’ and two groups from North Korea code-named ‘Zinc’ and ‘Cerium’. \"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f5FvzofgoNiy"
      },
      "outputs": [],
      "source": [
        "def display_inter_search_content(file_name):\n",
        "    try:\n",
        "        with open(f'saved_data/{file_name}', 'r') as file:\n",
        "            data = json.load(file)\n",
        "            content = \"\"\n",
        "            for entry in data:\n",
        "                if len(entry['relevance_annotations']) >1:\n",
        "                    query = \"Query: \" + entry[\"query\"]\n",
        "                    content += f\"<h3>{query}</h3>\"\n",
        "                    if entry[\"model_choice\"]:\n",
        "                        model_choice = \"Model Choice: \" + str(entry[\"model_choice\"])\n",
        "                        content += f\"<h4>{model_choice}</h4>\"\n",
        "                    if entry[\"session\"]:\n",
        "                        session_choice = \"Session_iD: \" + str(entry[\"session\"])\n",
        "                        content += f\"<h4>{session_choice}</h4>\"\n",
        "                    df = pd.DataFrame(entry[\"relevance_annotations\"])\n",
        "                    df.rename(columns={'doc_test': 'Doc_Test', 'translation': 'Translation', 'score': 'Score', 'rating': 'Rating'}, inplace=True)\n",
        "                    content += df.to_html(border=0, index=False)\n",
        "            return content\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def read_jsonl(file_path):\n",
        "    return pd.read_json(file_path, lines=True)\n",
        "\n",
        "def display_jsonl_content(file_name, session_id):\n",
        "    if file_name == 'interactive_search.json':\n",
        "        return display_inter_search_content(file_name)\n",
        "    try:\n",
        "        df = read_jsonl(f'saved_data/{file_name}')\n",
        "        if session_id and session_id != \"Everyone\":\n",
        "            df = df[df['Session'] == session_id]\n",
        "        content = \"\"\n",
        "        column_names = df.columns\n",
        "        for index, row in df.iterrows():\n",
        "            for col in column_names:\n",
        "                content += f\"<p><b>{col.capitalize()}:</b> {row[col]}</p>\"\n",
        "            content += \"<hr>\"\n",
        "        return content if content else \"No data found for this session ID\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def list_files(directory):\n",
        "    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "\n",
        "def get_unique_sessions(file_name):\n",
        "    try:\n",
        "        df = read_jsonl(f'{SAVED_DATA_DIRECTORY}/{file_name}')\n",
        "        return df['Session'].unique().tolist()\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "\n",
        "directory = f'{SAVED_DATA_DIRECTORY}/'\n",
        "\n",
        "\n",
        "def generate_cool_name():\n",
        "    name1 = coolname.generate_slug(2)+f'{random.randint(10, 99)}'\n",
        "    return name1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HeXZzGJy8s_f"
      },
      "outputs": [],
      "source": [
        "def write_to_jsonl(file_name, data):\n",
        "    data = json.dumps(data)\n",
        "    try:\n",
        "        with open(file_name, 'a+') as file:\n",
        "            file.write(data + \"\\n\")\n",
        "    except FileNotFoundError:\n",
        "        with open(file_name, 'w') as file:\n",
        "            json.dump(data, file)\n",
        "\n",
        "previous_values = {}\n",
        "\n",
        "\n",
        "def on_query_change(query, file_name, session):\n",
        "    file_name_log = directory + \"query_log.json\"\n",
        "    previous_query = previous_values.get(session, '')\n",
        "    previous_content_to_log = \"null\" if previous_query == '' else previous_query\n",
        "    data = {\n",
        "        'session': session,\n",
        "        'type': 'query',\n",
        "        'query_reformulator_type':file_name,\n",
        "        'previous_content': previous_content_to_log,\n",
        "        'current_content': query,\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "    write_to_jsonl(file_name_log, data)\n",
        "    previous_values[session] = query\n",
        "\n",
        "\n",
        "def save_relavance_annotations(session, query, document, rating):\n",
        "  if rating == \"\":\n",
        "    return\n",
        "  file_name_log = directory + \"relavance_annotations.json\"\n",
        "  data = {\n",
        "        'session': session,\n",
        "        'type': 'annotation',\n",
        "        'query': query,\n",
        "        'document': document,\n",
        "        'annotation': rating,\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "  write_to_jsonl(file_name_log, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LEPLUKv_tksu"
      },
      "outputs": [],
      "source": [
        "query_model_choices = ['castorini/doc2query-t5-base-msmarco','google/flan-t5-xxl'] #kd add whatever needs to be added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fzNG9-VDcrwI"
      },
      "outputs": [],
      "source": [
        "def display_session_name(session_id):\n",
        "    return session_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duFxEbITi2jM",
        "outputId": "4bf9eed7-620e-4a7e-b0fe-5800165a2dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:491: UserWarning: Theme should be a class loaded from gradio.themes\n",
            "  warnings.warn(\"Theme should be a class loaded from gradio.themes\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:43: UserWarning: You have unused kwarg parameters in Button, please remove them: {'size': 'sm'}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:43: UserWarning: You have unused kwarg parameters in Button, please remove them: {'size': 'sm', 'css': '.gradio-container { width: 2.5vw}'}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:43: UserWarning: You have unused kwarg parameters in Button, please remove them: {'size': 'sm', 'css': '.gradio-container { width: 5vw}'}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:951: UserWarning: Expected 1 arguments for function <function <lambda> at 0x78b0d8fa2680>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:955: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x78b0d8fa2680>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:951: UserWarning: Expected 1 arguments for function <function <lambda> at 0x78aec8321b40>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:955: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x78aec8321b40>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:951: UserWarning: Expected 1 arguments for function <function <lambda> at 0x78aec8321990>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:955: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x78aec8321990>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:43: UserWarning: You have unused kwarg parameters in Dropdown, please remove them: {'size': 'sm'}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:951: UserWarning: Expected 2 arguments for function <function display_jsonl_content at 0x78b004736dd0>, received 1.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:955: UserWarning: Expected at least 2 arguments for function <function display_jsonl_content at 0x78b004736dd0>, received 1.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(\"Query Tuner\") as demo:\n",
        "    session = gr.Textbox(info=\"Your\", visible=False)\n",
        "    demo.load(fn=generate_cool_name, inputs=None, outputs=session)\n",
        "    with gr.Tab(\"🤖 User Interface\"):\n",
        "        with gr.Row():\n",
        "          # QBE\n",
        "          prompt_input = gr.Textbox(label=\"Provide Example Document\",info=\"This document would be used to generate a query\",\n",
        "                                        value=sample_docs[0])\n",
        "          # Ad-hoc\n",
        "          interactive_query = gr.Textbox(value=(\" \"*len(sample_docs[0])), label=\"Search Query\", info=\"Search and Retrieve documents for one of the generated queries\", interactive=True)\n",
        "        with gr.Row():\n",
        "          with gr.Column():\n",
        "            btni3 = gr.Button(value =\"⚙️ Generate Query                    \",  size=\"sm\")\n",
        "            outputs3 = []\n",
        "            rds3 = []\n",
        "            for i in range(5):\n",
        "              with gr.Row():\n",
        "                with gr.Column():\n",
        "                  outputs3.append(gr.Textbox(value=\"\", label=f\"Query {str(i+1)}\", interactive=True))\n",
        "                  rds3.append(gr.Checkbox(label=\"Add to query\",elem_id=f\"cb{str(i)}\",interactive=True))\n",
        "          with gr.Column():\n",
        "            with gr.Row():\n",
        "              btn_reform = gr.Button(value=\"Reformulate\", size=\"sm\", css=\".gradio-container { width: 2.5vw}\")\n",
        "              btn2 = gr.Button(value=\"🔍 Search\", size=\"sm\", css=\".gradio-container { width: 5vw}\")\n",
        "            docs_en2 = []\n",
        "            rds4 = []\n",
        "            for i in range(NDOCS):\n",
        "              with gr.Row():\n",
        "                with gr.Column():\n",
        "                  dx = gr.Textbox(value=\"\",  label=f\"Document {str(i+1)}\", lines=3)\n",
        "                  docs_en2.append(dx)\n",
        "                  docs_en2.append(gr.Textbox(value=\"\", label=\"Translation\", lines=3))\n",
        "                  docs_en2.append(gr.Textbox(value=\"\", label=\"Score\", lines=3, visible=False))\n",
        "                  with gr.Row():\n",
        "                    slider = gr.Slider(minimum=0, maximum=10, label=f\"Rate Document {i+1}\", value=-1)\n",
        "                    slider.change(save_relavance_annotations, inputs=[session, interactive_query, dx, slider])\n",
        "                    docs_en2.append(slider)\n",
        "                    # docs_en2.append(gr.HighlightedText(value=\"\", label=f\"Event Annotations of Document {str(i+1)}\"))\n",
        "                    rds4.append(gr.Checkbox(label=\"Use this document to improve the query\", elem_id=f\"cb{str(i)}\", interactive=True))\n",
        "            for i in range(5):\n",
        "              rds3[i].select(fn=add_to_new_query, inputs=[interactive_query, outputs3[i]], outputs=interactive_query)\n",
        "            save_btn_inter = gr.Button(value=\"💾 Save Results\" , size=\"sm\", css=\".gradio-container { width: 5vw}\")\n",
        "        filename_input = gr.Textbox(value='interactive_search', visible=False)\n",
        "        btn2.click(lambda x:[False]*NDOCS, inputs=None, outputs=rds4)\n",
        "        btni3.click(lambda x:\"\", inputs=None, outputs=interactive_query)\n",
        "        btni3.click(lambda x:[False]*5, inputs=None, outputs=rds3)\n",
        "    with gr.Tab(\"⚙️ Configuration\"):\n",
        "        json_choice_list = ['interactive_search.json','query_log.json', 'relavance_annotations.json']\n",
        "        with gr.Row():\n",
        "            model_choice_query = gr.Dropdown(\n",
        "                label=\"Select Query Generator\",\n",
        "                choices = query_model_choices,\n",
        "                value=query_model_choices[1] ,\n",
        "                size=\"sm\"\n",
        "              )\n",
        "            prompt_instruction = gr.Dropdown(instructions,label=\"Choose Instruction\",info=\"Different instructions can invoke a variety of responses from LLMs\",value=instructions[1],interactive=True )\n",
        "            prompt_context = gr.Textbox(label=\"Sample Document-Query Pairs\",\n",
        "                                    placeholder=\"Provide text to generate query...\", value=dqpairs(ds_eg[1:], qs_eg[1:]),\n",
        "                                       lines=5)\n",
        "            btni3.click(query_generatortemp, inputs=[prompt_instruction,prompt_context, prompt_input,model_choice_query, session], outputs=[session,*outputs3])\n",
        "        with gr.Row():\n",
        "          reform_method = gr.Dropdown(['Zero-Shot QR', 'Few-Shot QR'],\n",
        "                                   label=\"Choose Type of Query Reformulator\",info=\"Additional keywords would be added to your original query\",\n",
        "                                   value='Zero-Shot QR')\n",
        "          reform_instruction = gr.Textbox(value=\"Suggest useful keywords to improve the retrieval effectiveness of the query: \", label=\"Reformulator Instruction\", info=\"Use the following instruction to reform the query inplace\", interactive=True)\n",
        "          reform_context = gr.Textbox(label=\"Sample Query-Reformed Query Pairs\",\n",
        "                                    placeholder=\"Provide text to generate query...\", value=\"\",\n",
        "                                       lines=5)\n",
        "        with gr.Row():\n",
        "            model_choice_retrival = gr.Dropdown(\n",
        "                label=\"Select retrieval pipeline to use\", info=\"The query would be run against the index to retrieve the top 10 documents.\",\n",
        "                choices=retrieval_algos_names,\n",
        "                value=retrieval_algos_names[0],\n",
        "              )\n",
        "            index_choice_retrieval = gr.Dropdown(\n",
        "                label=\"Select retrieval index to use\", info=\"The query would be run against this index.\",\n",
        "                choices=dataset_names,\n",
        "                value=dataset_names[3],\n",
        "              )\n",
        "            index_choice_retrieval.change(on_dataset_change, inputs=index_choice_retrieval)\n",
        "            btn2.click(document_retriever, inputs=[session, interactive_query, filename_input, model_choice_retrival], outputs=docs_en2)\n",
        "            btn2.click(save_document_auto, inputs = [session, model_choice_retrival,filename_input,interactive_query,*docs_en2])\n",
        "        with gr.Row():\n",
        "            dropdown = gr.Dropdown(choices=json_choice_list, label=\"Select JSON File\")\n",
        "            display_button = gr.Button(\"Display Recorded Annotations & Logs\")\n",
        "        output = gr.HTML()\n",
        "        model_choice_query.change(update_model, inputs=model_choice_query)\n",
        "        display_button.click(display_jsonl_content, inputs=dropdown, outputs=output)\n",
        "        save_btn_inter.click(save_document_auto, inputs = [session, model_choice_retrival,filename_input,interactive_query,*docs_en2])\n",
        "        btn_reform.click(append_keywords, inputs=[session, interactive_query, reform_method, reform_instruction, model_choice_query], outputs=interactive_query)\n",
        "        for i in range(NDOCS):\n",
        "              rds4[i].select(fn=send_feedback, inputs=[interactive_query, docs_en2[3*i + 1], model_choice_query, session], outputs=interactive_query)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IG7e97FNnnyr",
        "outputId": "97a23fda-9d05-4c4a-c34a-f5e2cd25ade7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://e84876c7d3f7a4ea8d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e84876c7d3f7a4ea8d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n",
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev:cuda\n",
            "dev:cuda\n",
            "dev:cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        }
      ],
      "source": [
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H_Mkpefo8MgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b261878-d203-458e-a05a-a7251e780561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67cb9f6a66e7475eb029e0e21c778fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_394ed19f676e436f9d323d50b17e2e27",
              "IPY_MODEL_1d4cbc87ab024c6e8908121b997fc2c4",
              "IPY_MODEL_072b7bdae8e84efd9e55a805181a7048"
            ],
            "layout": "IPY_MODEL_72c9f93238e9447c8ef5b1f661bc12fc"
          }
        },
        "394ed19f676e436f9d323d50b17e2e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e3bc5373fd946e89845351b81e64bfa",
            "placeholder": "​",
            "style": "IPY_MODEL_d8783bced142490bb60e31725c02e0c6",
            "value": "msmarco-passage/trec-dl-2019/judged documents: 100%"
          }
        },
        "1d4cbc87ab024c6e8908121b997fc2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8660ac017a6a4c14806e200aca11dd0a",
            "max": 8841823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4d06a5141434d80a7bccf4a4aaea2f0",
            "value": 8841823
          }
        },
        "072b7bdae8e84efd9e55a805181a7048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48b5e3564cd4424a85bd7f9dd357a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_da2671c6f4664171923ea4b598b54357",
            "value": " 8841823/8841823 [01:19&lt;00:00, 159078.46it/s]"
          }
        },
        "72c9f93238e9447c8ef5b1f661bc12fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3bc5373fd946e89845351b81e64bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8783bced142490bb60e31725c02e0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8660ac017a6a4c14806e200aca11dd0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4d06a5141434d80a7bccf4a4aaea2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e48b5e3564cd4424a85bd7f9dd357a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2671c6f4664171923ea4b598b54357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}